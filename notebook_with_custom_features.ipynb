{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)\n",
    "\n",
    "[Bonne segmentation des clients](https://www.kaggle.com/code/farrasalyafi/eda-customer-segmentatioin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce/download'\n",
    "zip_file_name = 'brazilian-e-commerce-dataset.zip'\n",
    "extract_name = 'olist_customers_dataset.csv'\n",
    "dl_path = 'downloads'\n",
    "zip_file_path = f'./{dl_path}/{zip_file_name}'\n",
    "extracts_path = 'extracts'\n",
    "extract_to_path = f'./{extracts_path}/{extract_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(zip_file_path):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile already downloaded\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Create the downloads folder\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(zip_file_path):\n",
    "    print('File already downloaded')\n",
    "else:\n",
    "    # Create the downloads folder\n",
    "    os.makedirs(dl_path)\n",
    "\n",
    "    # Download file from url\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(url, zip_file_path)\n",
    "    print('File downloaded')\n",
    "\n",
    "if os.path.isfile(extract_to_path):\n",
    "    print('File already extracted')\n",
    "else:\n",
    "    # Unzip file\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracts_path)\n",
    "    print('Files extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this schema to navigate between the different datasets :\n",
    "![dataset_relation_schema](https://i.imgur.com/HRhd2Y0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have to segment clients profiles, we will mostly use the client dataset (olist_order_customer_dataset) related to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid to explore every dataset, we will do only for data_customer then add a dataset to the exploration part every time we need one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading all the files\n",
    "raw_path = './extracts/'\n",
    "data_customer = pd.read_csv(raw_path + 'olist_customers_dataset.csv')\n",
    "data_geolocation = pd.read_csv(raw_path + 'olist_geolocation_dataset.csv')\n",
    "data_orders = pd.read_csv(raw_path + 'olist_orders_dataset.csv')\n",
    "data_order_items = pd.read_csv(raw_path + 'olist_order_items_dataset.csv')\n",
    "data_order_payments = pd.read_csv(raw_path + 'olist_order_payments_dataset.csv')\n",
    "data_order_reviews = pd.read_csv(raw_path + 'olist_order_reviews_dataset.csv')\n",
    "data_products = pd.read_csv(raw_path + 'olist_products_dataset.csv')\n",
    "data_sellers = pd.read_csv(raw_path + 'olist_sellers_dataset.csv')\n",
    "data_category = pd.read_csv(raw_path + 'product_category_name_translation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Menggabungkan semua data-data \n",
    "all_data = data_orders.merge(data_order_items, on='order_id', how='left')\n",
    "all_data = all_data.merge(data_order_payments, on='order_id', how='inner')\n",
    "all_data = all_data.merge(data_order_reviews, on='order_id', how='inner')\n",
    "all_data = all_data.merge(data_products, on='product_id', how='inner')\n",
    "all_data = all_data.merge(data_customer, on='customer_id', how='inner')\n",
    "all_data = all_data.merge(data_sellers, on='seller_id', how='inner')\n",
    "all_data = all_data.merge(data_category,on='product_category_name',how='inner')\n",
    "all_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st : data_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `customer_id` : key to the orders dataset. Each order has a unique customer_id.\n",
    "- `customer_unique_id` : unique identifier of a customer.\n",
    "- `customer_zip_code_prefix` : first five digits of customer zip code.\n",
    "- `customer_city` : customer city name.\n",
    "- `customer_state` : customer state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_customer.dtypes)\n",
    "data_customer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "number_of_orders = Counter(data_customer['customer_unique_id'].value_counts().values)\n",
    "pd_number_of_orders = pd.DataFrame(number_of_orders.items(), columns=['number_of_order_per_customer', 'occurences'])\n",
    "# keys = number_of_orders.keys()\n",
    "# values = number_of_orders.values()\n",
    "\n",
    "ax = sns.barplot(x='number_of_order_per_customer', y = 'occurences', data = pd_number_of_orders)\n",
    "ax.set(xlabel='Number of orders', ylabel='Occurences')\n",
    "ax.set_title('Number of orders per customer frequence')\n",
    "ax.set_ylim(0, 100000)\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_customer[data_customer.isnull().sum(axis=1) != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oultiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ids seemed to be in hex with a length of 32, we check that they all fulfill this condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "if data_customer.loc[data_customer['customer_unique_id'].str.len() != 32,:].shape[0] == 0:\n",
    "    print('All the customer_unique_id are 32 characters long')\n",
    "else:\n",
    "    print('There are some customer_unique_id which are not 32 characters long')\n",
    "\n",
    "try:\n",
    "    data_customer['customer_unique_id'].apply(int, base=16)\n",
    "    print('All the customer_unique_id are in base 16')\n",
    "except:\n",
    "    print('There are some customer_unique_id which are not in base 16')\n",
    "    \n",
    "if data_customer.loc[data_customer['customer_id'].str.len() != 32,:].shape[0] == 0:\n",
    "    print('All the customer_id are 32 characters long')\n",
    "else:\n",
    "    print('There are some customer_id which are not 32 characters long')\n",
    "\n",
    "try:\n",
    "    data_customer['customer_id'].apply(int, base=16)\n",
    "    print('All the customer_id are in base 16')\n",
    "except:\n",
    "    print('There are some customer_id which are not in base 16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other columns on the dataset are not interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd : data_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `order_id` : unique identifier of the order.\n",
    "- `customer_id` : key to the customer dataset. Each order has a unique customer_id.\n",
    "- `order_status` : Reference to the order status (delivered, shipped, etc).\n",
    "- `order_purchase_timestamp` : Shows the purchase timestamp.\n",
    "- `order_approved_at` : Shows the payment approval timestamp.\n",
    "- `order_delivered_carrier_date` : Shows the order posting timestamp. When it was handled to the logistic partner.\n",
    "- `order_delivered_customer_date` : Shows the actual order delivery date to the customer.\n",
    "- `order_estimated_delivery_date` : Shows the estimated delivery date that was informed to customer at the purchase moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_orders.dtypes)\n",
    "data_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of dates in this dataframe, we will check their distribution by grouping them per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orders_date_columns = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "data_order_visualization = data_orders[data_orders_date_columns]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(len(data_orders_date_columns),3,gridspec_kw={'width_ratios': [2, 4, 2]}, figsize=(12,15))\n",
    "\n",
    "for i, column in enumerate(data_orders_date_columns):\n",
    "    \n",
    "    sorted_dates = pd.to_datetime(data_order_visualization[column]).dt.strftime(' %y-%m').dropna()\n",
    "    sorted_dates = sorted_dates.sort_values(ascending=True)\n",
    "    hundred_first_dates = sorted_dates.head(round(len(sorted_dates)*0.1))\n",
    "    hundred_last_dates = sorted_dates.tail(round(len(sorted_dates)*0.1))\n",
    "\n",
    "    axs[i,0].hist(hundred_first_dates, bins=20)\n",
    "    axs[i,0].set_title('First 10% of ' + column)\n",
    "    axs[i,0].set_xlabel(column)\n",
    "    axs[i,0].tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    axs[i,1].hist(sorted_dates, bins=40)\n",
    "    axs[i,1].set_title('All dates of ' + column)\n",
    "    axs[i,1].set_xlabel(column)\n",
    "    axs[i,1].set_ylabel('Frequency')\n",
    "    axs[i,1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    axs[i,2].hist(hundred_last_dates, bins=20)\n",
    "    axs[i,2].set_title('Last 10% of ' + column)\n",
    "    axs[i,2].set_xlabel(column)\n",
    "    axs[i,2].set_ylabel('Frequency')\n",
    "    axs[i,2].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oultiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it could be reasonneable to truncate the date from 01 Mar 2017 to 30 Sep 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_datetime = pd.to_datetime('2017-03-01')\n",
    "max_datetime = pd.to_datetime('2018-09-30')\n",
    "\n",
    "# Convert the date to datetime format\n",
    "data_orders[data_orders_date_columns] = data_orders[data_orders_date_columns].apply(pd.to_datetime)\n",
    "\n",
    "# If one of the colums is over the max date or under the min date, then we delete the order\n",
    "conditions = (\n",
    "    (data_orders['order_purchase_timestamp'] > max_datetime)\n",
    "    | (pd.to_datetime(data_orders['order_purchase_timestamp']) <  min_datetime)\n",
    "    | (data_orders['order_approved_at'] > max_datetime)\n",
    "    | (pd.to_datetime(data_orders['order_approved_at']) <  min_datetime)\n",
    "    | (data_orders['order_delivered_carrier_date'] > max_datetime)\n",
    "    | (pd.to_datetime(data_orders['order_delivered_carrier_date']) <  min_datetime)\n",
    "    | (data_orders['order_delivered_customer_date'] > max_datetime)\n",
    "    | (pd.to_datetime(data_orders['order_delivered_customer_date']) <  min_datetime)\n",
    "    | (data_orders['order_estimated_delivery_date'] > max_datetime)\n",
    "    | (pd.to_datetime(data_orders['order_estimated_delivery_date']) <  min_datetime)\n",
    ")\n",
    "\n",
    "orders_to_delete = data_orders[conditions]\n",
    "print(f\"{len(orders_to_delete)} orders will be deleted\")\n",
    "data_orders.drop(orders_to_delete.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_order_visualization = data_orders[data_orders_date_columns]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(len(data_orders_date_columns),3,gridspec_kw={'width_ratios': [2, 4, 2]}, figsize=(12,15))\n",
    "\n",
    "for i, column in enumerate(data_orders_date_columns):\n",
    "    \n",
    "    sorted_dates = pd.to_datetime(data_order_visualization[column]).dt.strftime(' %y-%m').dropna()\n",
    "    sorted_dates = sorted_dates.sort_values(ascending=True)\n",
    "    hundred_first_dates = sorted_dates.head(round(len(sorted_dates)*0.1))\n",
    "    hundred_last_dates = sorted_dates.tail(round(len(sorted_dates)*0.1))\n",
    "\n",
    "    axs[i,0].hist(hundred_first_dates, bins=20)\n",
    "    axs[i,0].set_title('First 10% of ' + column)\n",
    "    axs[i,0].set_xlabel(column)\n",
    "    axs[i,0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    axs[i,1].hist(sorted_dates, bins=40)\n",
    "    axs[i,1].set_title('All dates of ' + column)\n",
    "    axs[i,1].set_xlabel(column)\n",
    "    axs[i,1].set_ylabel('Frequency')\n",
    "    axs[i,1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    axs[i,2].hist(hundred_last_dates, bins=20)\n",
    "    axs[i,2].set_title('Last 10% of ' + column)\n",
    "    axs[i,2].set_xlabel(column)\n",
    "    axs[i,2].set_ylabel('Frequency')\n",
    "    axs[i,2].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_orders.loc[data_orders['order_id'].str.len() != 32,:].shape[0] == 0:\n",
    "    print('All the order_id are 32 characters long')\n",
    "else:\n",
    "    print('There are some order_id which are not 32 characters long')\n",
    "\n",
    "try:\n",
    "    data_orders['order_id'].apply(int, base=16)\n",
    "    print('All the order_id are in base 16')\n",
    "except:\n",
    "    print('There are some order_id which are not in base 16')\n",
    "    \n",
    "if data_orders.loc[data_orders['customer_id'].str.len() != 32,:].shape[0] == 0:\n",
    "    print('All the customer_id are 32 characters long')\n",
    "else:\n",
    "    print('There are some customer_id which are not 32 characters long')\n",
    "\n",
    "try:\n",
    "    data_orders['customer_id'].apply(int, base=16)\n",
    "    print('All the customer_id are in base 16')\n",
    "except:\n",
    "    print('There are some customer_id which are not in base 16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd : data_order_payments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `order_id` : unique identifier of the order.\n",
    "- `payment_sequential` : a customer may pay an order with more than one payment method. If he does so, a sequence will be created to accommodate all payments.\n",
    "- `payment_type` : method of payment chosen by the customer.\n",
    "- `payment_installments` : number of installments chosen by the customer.\n",
    "- `payment_value` : transaction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_order_payments.dtypes)\n",
    "data_order_payments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_payments = data_order_payments['payment_sequential'].value_counts()\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "barplot = ax.bar(x = sequential_payments.index, height=sequential_payments.values)\n",
    "ax.bar_label(barplot)\n",
    "ax.set_title('Payment sequential')\n",
    "ax.set_xlabel('Payment sequential')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlim(0,30)\n",
    "ax.set_xticks(sequential_payments.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "to_pie_plot_columns = ['payment_sequential', 'payment_type', 'payment_installments']\n",
    "others_percent_threshold = 3\n",
    "\n",
    "fig, ax = plt.subplots(1, len(to_pie_plot_columns), figsize=(12,6))\n",
    "for i, column in enumerate(to_pie_plot_columns):\n",
    "    values_counts = data_order_payments[column].value_counts()\n",
    "    total_values = values_counts.sum()\n",
    "    others_index = values_counts[values_counts/total_values*100 < others_percent_threshold].index\n",
    "    values_others = values_counts[others_index]\n",
    "    values_counts = values_counts.drop(others_index)\n",
    "    others_sum = values_others.sum()\n",
    "\n",
    "    values_counts = values_counts.reset_index()\n",
    "    values_counts['index'] = values_counts['index'].astype(str)\n",
    "    values_counts = values_counts.append({'index':'others', column:others_sum}, ignore_index=True)\n",
    "\n",
    "\n",
    "    ax[i].pie(x=values_counts[column], labels =values_counts['index'] , autopct='%1.1f%%')\n",
    "    ax[i].set_title(column)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,gridspec_kw={'width_ratios': [2, 4, 2]}, figsize=(12,4))\n",
    "\n",
    "payments_sorted = data_order_payments['payment_value'].sort_values(ascending=True)\n",
    "nb_payments = len(payments_sorted)\n",
    "ten_percent_first = payments_sorted.head(round(0.1 * nb_payments))\n",
    "ten_percent_last = payments_sorted.tail(round(0.1 * nb_payments))\n",
    "\n",
    "axs[0].hist(ten_percent_first, bins=20)\n",
    "axs[0].set_title('10% lowest payments')\n",
    "axs[0].tick_params(rotation=90)\n",
    "axs[0].set_xlim(0, max(ten_percent_first))\n",
    "\n",
    "axs[1].hist(payments_sorted, bins=40)\n",
    "axs[1].set_title('All payments')\n",
    "axs[1].tick_params(rotation=90)\n",
    "axs[1].set_xlim(min(payments_sorted), max(payments_sorted))\n",
    "\n",
    "axs[2].hist(ten_percent_last, bins=20)\n",
    "axs[2].set_title('10% highests payments')\n",
    "axs[2].tick_params(rotation=90)\n",
    "axs[2].set_xlim(min(ten_percent_last), max(ten_percent_last))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feuille de route :\n",
    "\n",
    "1. Faire un premier jeu de variables avec RFM --> Date de la dernière commande | Nombre de commandes | Total dépensé\n",
    "2. Faire le clustering sur ces variables sur une période fixe (toute l'année) puis du flotant\n",
    "3. Rajouter les features et refaire un clustering\n",
    "4. Rajouter plusieurs algorithmes\n",
    "5. Faire de l'interpretation en pensant au point de vue marketing\n",
    "\n",
    "Features interessantes :\n",
    "- Temps médian entre les commandes\n",
    "- Somme médianne des commandes\n",
    "- Mois avec le plus de commandes (avec une pondération ?)\n",
    "- Catégorie préférée (attention, variable catégorielle)\n",
    "- Score moyen des reviews\n",
    "- \"A laissé une review\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new features to a custom dataframe representing a customer.\n",
    "Adding :\n",
    "- **Recency** (How recently did the customer purchase?) : When was the last order\n",
    "- **Frequency** (How often do they purchase?) : Average time between orders (time signed up / number of orders)\n",
    "- **Monetary Value** (How much do they spend?) : What is the median of the order value\n",
    "- **number of orders** : How many orders the customer has made.\n",
    "- **customer's seniority** : How long the customer has been a customer.\n",
    "- **total spent** : How much the customer has spent\n",
    "- **prefered category** : What is the prefered category of the customer\n",
    "- **most spent month** : Month with the most orders\n",
    "- **average review score** : Average review score of the customer\n",
    "  \n",
    "Every date is transformed into a number of days from now. And we consider (even if it isn't mentionned) that the times are in UTC.\n",
    "Also, in order to calculate the customer's seniority, we consider the current date as the 01/01/2019 because the dataset gathers data from 2016 to 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_customers_df = data_customer.drop(['customer_zip_code_prefix', 'customer_city', 'customer_state', 'customer_id'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recency** (How recently did the customer purchase?) : When was the last order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Keeping the relation from order customer id and customer unique id\n",
    "custom_orders_df = data_customer[['customer_unique_id', 'customer_id']]\n",
    "# Getting the orders details\n",
    "custom_orders_df = custom_orders_df.merge(data_orders, on='customer_id', how='inner')\n",
    "# Grouping every customer by their customer_unique_id and selecting the most recent order date (and renaming it)\n",
    "custom_orders_most_recent_df = custom_orders_df.groupby('customer_unique_id').agg({'order_purchase_timestamp': 'max'}).reset_index()\n",
    "custom_orders_most_recent_df['Recency'] = (max_datetime - pd.to_datetime(custom_orders_most_recent_df['order_purchase_timestamp'], format='%Y-%m-%d %H:%M:%S')).dt.days\n",
    "custom_orders_most_recent_df = custom_orders_most_recent_df.drop('order_purchase_timestamp', axis=1)\n",
    "\n",
    "custom_orders_most_recent_df.sort_values(by='Recency')\n",
    "\n",
    "# Adding recency column to the custom_customers_df\n",
    "custom_customers_df = custom_customers_df.merge(custom_orders_most_recent_df, on='customer_unique_id', how='inner')\n",
    "\n",
    "\n",
    "custom_customers_df.sort_values(by='Recency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_per_customer = custom_orders_df['customer_unique_id'].value_counts()\n",
    "orders_per_customer = pd.DataFrame(orders_per_customer.items(), columns=['customer_unique_id', 'number_of_orders'])\n",
    "custom_customers_df = custom_customers_df.merge(orders_per_customer, on='customer_unique_id', how='inner')\n",
    "custom_customers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monetary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_orders_df[['customer_unique_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spent_per_customer = custom_orders_df['customer_unique_id'].value_counts()\n",
    "total_spent_per_customer = pd.DataFrame(total_spent_per_customer.items(), columns=['customer_unique_id', 'number_of_orders'])\n",
    "custom_customers_df = custom_customers_df.merge(orders_per_customer, on='customer_unique_id', how='inner')\n",
    "custom_customers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency (and seniority and number of orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency** (How often do they purchase?) : Average time between orders (customer's seniority / number of orders)\n",
    "\n",
    "**Number of orders** : How many orders the customer has made.\n",
    "\n",
    "**Customer's seniority** : How long the customer has been a customer.\n",
    "\n",
    "Because there is no information about one customer's first activity, we will use the first order date as the customer's first activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse of the previous function for getting the most recent order date, but for the oldest order\n",
    "custom_orders_most_ancient_df = custom_orders_df.groupby('customer_unique_id').agg({'order_purchase_timestamp': 'min'}).reset_index().rename(columns={'order_purchase_timestamp': 'Seniority'})\n",
    "custom_customers_df = custom_customers_df.merge(custom_orders_most_ancient_df, on='customer_unique_id', how='inner')\n",
    "custom_customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_per_customer = custom_orders_df['customer_unique_id'].value_counts()\n",
    "orders_per_customer = pd.DataFrame(orders_per_customer.items(), columns=['customer_unique_id', 'number_of_orders'])\n",
    "custom_customers_df = custom_customers_df.merge(orders_per_customer, on='customer_unique_id', how='inner')\n",
    "custom_customers_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25cbe7383e98738c94ec2eff7267cb5a2e6045e4b727232eee56ddb015707007"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
